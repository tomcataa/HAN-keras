{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 gpu memory 動態成長\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42120, 2448, 8062)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"\")\n",
    "df_train = df[df[\"source\"] == \"train\"].copy()\n",
    "df_dev = df[df[\"source\"] == \"dev\"].copy()\n",
    "df_test = df[df[\"source\"] == \"test\"].copy()\n",
    "len(df_train), len(df_dev), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(sentences):\n",
    "    # session裡去掉0:、1:開頭、斷詞\n",
    "    sentences = sentences.split(\"\\n\")\n",
    "    sentences = [row[1:].strip() if re.match(\"[0-9]\", row[0]) else row for row in sentences]\n",
    "    sentences = [\" \".join(list(row.replace(\" \", \"\"))) for row in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 變成tuple是為了排序\n",
    "train_list = [] # list of tuple: [ ([\"first sentence\", \"second sentence\"], l1_label), ... ]\n",
    "dev_list = []\n",
    "test_list = []\n",
    "\n",
    "train_list = [(get_content(row[\"sentences\"]), row[\"l1_label\"]) for index, row in df_train.iterrows()]\n",
    "dev_list = [(get_content(row[\"sentences\"]), row[\"l1_label\"]) for index, row in df_dev.iterrows()]\n",
    "test_list = [(get_content(row[\"sentences\"]), row[\"l1_label\"]) for index, row in df_test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by number of sentences\n",
    "train_list.sort(key=lambda s: len(s[0]), reverse=True)\n",
    "dev_list.sort(key=lambda s: len(s[0]), reverse=True)\n",
    "test_list.sort(key=lambda s: len(s[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子部份\n",
    "train_text_list = [r[0] for r in train_list]\n",
    "dev_text_list = [r[0] for r in dev_list]\n",
    "test_text_list = [r[0] for r in test_list]\n",
    "\n",
    "# label部份\n",
    "train_label_list = [r[1] for r in train_list]\n",
    "dev_label_list = [r[1] for r in dev_list]\n",
    "test_label_list = [r[1] for r in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_list[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = [len(row.split()) for text in train_text_list for row in text]\n",
    "pd.DataFrame(sent_len, columns=[\"所有句子長度分布\"]).describe([0, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_count = 3 # 最少出現次數才要算入vocabulary\n",
    "PAD_INDEX = 0\n",
    "UNK_INDEX = 1\n",
    "START_INDEX = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "word_index = dict()      # word: index\n",
    "label_index = dict()     # label: class\n",
    "inv_label_index = dict() # class: label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算字出現的次數\n",
    "for text in train_text_list: # for each document\n",
    "    for row in text:         # for each row in a document\n",
    "        for w in row.split():# for each word in a row\n",
    "            word_counter[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 559679), ('是', 503225), ('个', 499197), ('您', 466854), ('我', 448510)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2946 words\n"
     ]
    }
   ],
   "source": [
    "# 出現次數 >= min_word_count 的字進到 vocabulary(word_index)\n",
    "word_index = dict()\n",
    "index = START_INDEX\n",
    "for w, v in word_counter.most_common():\n",
    "    if v < min_word_count:\n",
    "        break\n",
    "    word_index[w] = index\n",
    "    index += 1\n",
    "print(\"total %d words\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder\n",
    "label_index = {v:k for k, v in enumerate(set(df_train[\"l1_label\"]))}\n",
    "print(label_index)\n",
    "inv_label_index = {v:k for k, v in label_index.items()}\n",
    "print(inv_label_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(sents):\n",
    "    \"\"\"\n",
    "    字->index\n",
    "    input: 1D array: a document(list of sentences) space separated. eg: [\"w11 w12\", \"w21, w22, w23\", .. ]\n",
    "    return 2D array: shape=(sentence_len, word_len)\n",
    "    \"\"\"\n",
    "    feature = []\n",
    "    for row in sents:\n",
    "        seq = [word_index.get(w, UNK_INDEX) for w in row.split()]\n",
    "        feature.append(seq)\n",
    "            \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_seq = [get_feature(s) for s in train_text_list]\n",
    "dev_seq = [get_feature(s) for s in dev_text_list]\n",
    "test_seq = [get_feature(s) for s in test_text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y = np.array([label_index[w] for w in train_label_list])\n",
    "dev_y = np.array([label_index[w] for w in dev_label_list])\n",
    "test_y = np.array([label_index[w] for w in test_label_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrain embedding weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pretrain_emb/1.6m_cleaned_cbow_mincount_5_window_5_cbowmean_1.vec\") as fr:\n",
    "    data = fr.read().strip().split(\"\\n\")[2:]\n",
    "    print(\"%d words\" % len(data))\n",
    "    \n",
    "    word_weight = dict()\n",
    "    for row in data:\n",
    "        w = row.split()\n",
    "        word_weight[w[0]] = [float(i) for i in w[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_emb = np.random.uniform(-0.05, 0.05, (len(word_index)+2, 300))\n",
    "count = 0\n",
    "for word, index in word_index.items():\n",
    "    if word in word_weight:\n",
    "        pretrain_emb[index] = word_weight[word]\n",
    "        count += 1\n",
    "print(\"{} / {} = {:.4f} has pretrained weight\".format(count, len(word_index), count/len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_embedding(x, vocab_size):\n",
    "    import tensorflow as tf\n",
    "    e = np.eye(vocab_size)\n",
    "    e[0, 0] = 0\n",
    "    one_hot_array = tf.constant(e, dtype=\"float32\")\n",
    "    return tf.nn.embedding_lookup(one_hot_array, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size = pretrain_emb.shape[1]\n",
    "embedding_size = 200\n",
    "hidden_size = 50\n",
    "uw_size = 100\n",
    "us_size = 100\n",
    "batch_size = 64\n",
    "max_sent_len = 50 # 一句最多看50個字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 先做word-level的model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個subgraph的   \n",
    "input是一個句子. eg: ```[30, 288, 7, 0, ..., 0]```   \n",
    "output是這個句子的representation. 即每個word_representation 的 weighted sum. 在這裡```output shape=(100,)```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def word_attention(inputs):\n",
    "    \"\"\"\n",
    "    inputs = (time_steps, uw_size)\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # inputs = (batch, time_steps, uw_size) = (None, None, 100)\n",
    "    \n",
    "    uw = tf.Variable(tf.random_uniform(shape=(uw_size, ), dtype=tf.float32))\n",
    "    \n",
    "    # inputs * uw = (batch, time_steps, uw_size) = (None, None, 100)\n",
    "    m = tf.reduce_sum(inputs * uw, axis=-1) # (batch, time_steps) = (None, None)\n",
    "    \n",
    "    alpha = tf.nn.softmax(m)\n",
    "    # tf.expand_dims(alpha, axis=-1) = (batch, time_steps, 1)\n",
    "    s = inputs * tf.expand_dims(alpha, axis=-1) # (batch, time_steps, uw_size) = (None, None, 100)\n",
    "    output = tf.reduce_mean(s, axis=1) # (batch, uw_size) = (None, 100)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.att_size = kwargs.pop(\"att_size\")\n",
    "        self.output_dim = kwargs.pop(\"output_dim\")\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.uw = self.add_weight(\n",
    "            name=\"attention_size\", \n",
    "            shape=(self.att_size, ),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True)\n",
    "        super(Attention, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "        \n",
    "    def call(self, x):\n",
    "        m = tf.reduce_sum(x * self.uw, axis=-1)\n",
    "        alpha = tf.nn.softmax(m)\n",
    "        s = x * tf.expand_dims(alpha, axis=-1)\n",
    "        return tf.reduce_mean(s, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "    \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"att_size\": self.att_size,\n",
    "            \"output_dim\": self.output_dim,\n",
    "        }\n",
    "        base_config = super(Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape = number of words\n",
    "input_sentence_i = Input(shape=(None,), name=\"input_sentence_i\", dtype=\"int32\")\n",
    "\n",
    "embedding = Embedding(len(word_index)+2, embedding_size, mask_zero=True,\n",
    "                      name=\"word_embedding\")(input_sentence_i)\n",
    "# embedding = Lambda(\n",
    "#     one_hot_embedding, output_shape=(None, len(word_index)+2),\n",
    "#     arguments={\"vocab_size\": len(word_index)+2}, input_shape=(None,))(input_sentence_i)\n",
    "\n",
    "# output shape = (time_steps, hidden_size*2). 在這裡 time_steps = None\n",
    "word_h_seq = Bidirectional(GRU(hidden_size, return_sequences=True), name=\"word_h_seq\")(embedding)\n",
    "\n",
    "# 用 TimeDistributed 對 input=(time_steps, hidden_size*2)的每個 time_steps apply a same Dense layer\n",
    "# output shape = (time_steps, uw_size)\n",
    "word_mlp = TimeDistributed(Dense(uw_size, activation=\"tanh\"), name=\"word_mlp\")(word_h_seq)\n",
    "\n",
    "# 將 word hidden state sequence 丟進去算 attention，得到這句 input_sentence_i representation\n",
    "# si_representation = Lambda(word_attention, output_shape=(hidden_size*2,), name=\"si_representation\")(word_mlp)\n",
    "# si_representation = Attention(att_size=uw_size, output_dim=hidden_size*2, name=\"si_representation\")(word_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_representation = Lambda(\n",
    "    lambda x: tf.reduce_mean(x, axis=1), output_shape=(uw_size, ), name=\"si_representation\")(word_mlp)\n",
    "# sent_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_encoder = Model(inputs=input_sentence_i, outputs=si_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_sentence_i (InputLayer (None, None)              0         \n",
      "_________________________________________________________________\n",
      "word_embedding (Embedding)   (None, None, 200)         589600    \n",
      "_________________________________________________________________\n",
      "word_h_seq (Bidirectional)   (None, None, 100)         75300     \n",
      "_________________________________________________________________\n",
      "word_mlp (TimeDistributed)   (None, None, 100)         10100     \n",
      "_________________________________________________________________\n",
      "si_representation (Lambda)   (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 675,000\n",
      "Trainable params: 675,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sent_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(sent_encoder).create(prog=\"dot\", format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentence-level model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input是一篇文章. eg: ```[ [30, 288, 7, 0, ..., 0], [10, 30, ..., 0, 0], .., [0, 0, ..., 0] ]```   \n",
    "最後會得到這篇文章的representation. 即每個sentence representation 的 weighted sum.   \n",
    "output是doc_repre + Dense去分類. ```output shape=(label_size,)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape = (num_sentences, num_words=70)\n",
    "input_document = Input(shape=(None, max_sent_len), name=\"input_document\", dtype=\"int32\")\n",
    "\n",
    "# output shape = (time_steps, si_representation=hidden_size*2) 在這裡 time_steps = 句子數 = document length = None\n",
    "# 把每一句丟到 sent_encoder，得到這句的 sentence representation\n",
    "sentence_representation = TimeDistributed(sent_encoder, name=\"sentence_representation\")(input_document)\n",
    "\n",
    "# output shape = (time_steps, hidden_size*2)\n",
    "sent_h_seq = Bidirectional(GRU(hidden_size, return_sequences=True), name=\"sent_h_seq\")(sentence_representation)\n",
    "\n",
    "# output shape = (time_steps, us_size)\n",
    "sent_mlp = TimeDistributed(Dense(us_size, activation=\"tanh\"), name=\"sent_mlp\")(sent_h_seq)\n",
    "\n",
    "# 將 sentence hidden state sequence 丟進去算 attention，得到這篇 input_document representation\n",
    "# doc_representation = Lambda(sentence_attention, output_shape=(hidden_size*2,), name=\"doc_representation\")(sent_mlp)\n",
    "# doc_representation = Attention(att_size=us_size, output_dim=hidden_size*2, name=\"doc_representation\")(sent_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_representation = Lambda(\n",
    "    lambda x: tf.reduce_mean(x, axis=1), output_shape=(us_size, ), name=\"doc_representation\")(sent_mlp)\n",
    "# doc_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Dense(len(label_index), activation=\"softmax\", name=\"output\")(doc_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_document, outputs=predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_document (InputLayer)  (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "sentence_representation (Tim (None, None, 100)         675000    \n",
      "_________________________________________________________________\n",
      "sent_h_seq (Bidirectional)   (None, None, 100)         45300     \n",
      "_________________________________________________________________\n",
      "sent_mlp (TimeDistributed)   (None, None, 100)         10100     \n",
      "_________________________________________________________________\n",
      "doc_representation (Lambda)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 730,804\n",
      "Trainable params: 730,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model).create(prog=\"dot\", format=\"svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save(\"test.h5\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = keras.models.load_model(\"test.h5\", custom_objects={\"Attention\": Attention})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 每個batch內的document pad到相同句子數，所有句子pad到相同長度(=max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size):\n",
    "    # 取得每個 batch 要 train 的 index\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches_for_one_hot(n, minibatch_size):\n",
    "    # 取得每個 batch 要 train 的 index\n",
    "    # for one hot 是因為為了一二篇超長文章，把整個 batch pad 到最長會導致 gpu oom\n",
    "    # 前2個 batch 改成 batch_size=8 試試看\n",
    "    \n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    \n",
    "    long_minibatches = []\n",
    "    for i in range(2): # 要拆掉幾個 minibatches\n",
    "        mini = minibatches.pop(0)\n",
    "        batches = np.split(mini, 8)\n",
    "        long_minibatches += batches\n",
    "    \n",
    "    return long_minibatches + minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(sequences, labels, minibatches):\n",
    "    x = [] # 一個element是之後要train的一個batch\n",
    "    y = [] # 一個element是之後要train的一個batch\n",
    "    \n",
    "    for idx in minibatches:\n",
    "        r = []\n",
    "        max_doc_len = max([len(sequences[i]) for i in idx]) # 文章長度 = 這個batch裡最多句子數\n",
    "        for i in range(len(idx)):\n",
    "            sample = np.zeros((max_doc_len, max_sent_len), dtype=\"int32\") # padding，一句的長度固定是 max_sent_len=50\n",
    "            seq = sequences[idx[i]]\n",
    "            for j in range(len(seq)):\n",
    "                sample[j, :len(seq[j])] = seq[j][:max_sent_len]\n",
    "            r.append(sample)\n",
    "        x.append(np.array(r))\n",
    "        y.append(np.array(labels[idx]))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 659 batches\n"
     ]
    }
   ],
   "source": [
    "minibatches = get_minibatches(len(tr_seq), batch_size)\n",
    "print(\"train: %d batches\" % len(minibatches))\n",
    "batch_tr_x, batch_tr_y = gen_samples(tr_seq, tr_y, minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatches[0].shape, minibatches[1].shape, minibatches[15].shape, minibatches[16].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatches[0], minibatches[1], minibatches[15], minibatches[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev: 153 batches\n"
     ]
    }
   ],
   "source": [
    "minibatches = get_minibatches(len(dev_seq), 16)\n",
    "print(\"dev: %d batches\" % len(minibatches))\n",
    "batch_dev_x, batch_dev_y = gen_samples(dev_seq, dev_y, minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 504 batches\n"
     ]
    }
   ],
   "source": [
    "minibatches = get_minibatches(len(test_seq), 16)\n",
    "print(\"test: %d batches\" % len(minibatches))\n",
    "batch_test_x, batch_test_y = gen_samples(test_seq, test_y, minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluate(m, bx, by):\n",
    "    # bx, by: batch_bucket\n",
    "    \n",
    "    losses = []\n",
    "    right = 0\n",
    "    for i in range(len(bx)):\n",
    "        l, r = m.evaluate(bx[i], by[i], verbose=0)\n",
    "        losses.append(l)\n",
    "        right += r\n",
    "        \n",
    "    return sum(losses)/len(losses), right/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:57<00:00,  2.78it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0. train loss: 0.618052 acc: 0.743124. dev loss: 0.618839 acc: 0.746732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:54<00:00,  2.81it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1. train loss: 0.551910 acc: 0.771932. dev loss: 0.627221 acc: 0.742647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:54<00:00,  2.80it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2. train loss: 0.521009 acc: 0.787344. dev loss: 0.586805 acc: 0.745507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:54<00:00,  2.80it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3. train loss: 0.474337 acc: 0.808446. dev loss: 0.579806 acc: 0.759395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:55<00:00,  2.80it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4. train loss: 0.445960 acc: 0.824308. dev loss: 0.557102 acc: 0.765523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:54<00:00,  2.81it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5. train loss: 0.414821 acc: 0.838534. dev loss: 0.578401 acc: 0.761029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:54<00:00,  2.81it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6. train loss: 0.410433 acc: 0.841284. dev loss: 0.641300 acc: 0.732026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:55<00:00,  2.79it/s]\n",
      "  0%|          | 1/659 [00:00<01:28,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7. train loss: 0.384009 acc: 0.850318. dev loss: 0.671849 acc: 0.743873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:54<00:00,  2.81it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8. train loss: 0.340540 acc: 0.873459. dev loss: 0.618790 acc: 0.759804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:56<00:00,  2.79it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9. train loss: 0.315649 acc: 0.884626. dev loss: 0.623527 acc: 0.768791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:54<00:00,  2.81it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10. train loss: 0.281541 acc: 0.897548. dev loss: 0.656743 acc: 0.767974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:54<00:00,  2.81it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11. train loss: 0.293235 acc: 0.891526. dev loss: 0.656238 acc: 0.764706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:55<00:00,  2.80it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12. train loss: 0.234018 acc: 0.919291. dev loss: 0.745364 acc: 0.741830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:54<00:00,  2.81it/s]\n",
      "  0%|          | 0/659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13. train loss: 0.224572 acc: 0.920879. dev loss: 0.769269 acc: 0.749592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [03:55<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14. train loss: 0.192161 acc: 0.936220. dev loss: 0.829379 acc: 0.736111\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "log_folder = \"model8/run3\"\n",
    "tr_loss = []\n",
    "tr_acc = []\n",
    "dev_loss = []\n",
    "dev_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 隨機排序batch index\n",
    "    p = np.random.permutation(len(batch_tr_x))\n",
    "    for i in tqdm(p):\n",
    "#     for i in range(len(batch_tr_x)):\n",
    "#         print(i)\n",
    "        model.train_on_batch(batch_tr_x[i], batch_tr_y[i])\n",
    "    \n",
    "    model.save(log_folder + \"/model.{}.h5\".format(epoch))\n",
    "    \n",
    "    loss, acc = run_evaluate(model, batch_tr_x, batch_tr_y)\n",
    "    tr_loss.append(loss)\n",
    "    tr_acc.append(acc)\n",
    "    \n",
    "    loss, acc = run_evaluate(model, batch_dev_x, batch_dev_y)\n",
    "    dev_loss.append(loss)\n",
    "    dev_acc.append(acc)\n",
    "    \n",
    "    print(\"epoch {}. train loss: {:.6f} acc: {:.6f}. dev loss: {:.6f} acc: {:.6f}\".format(\n",
    "        epoch, tr_loss[-1], tr_acc[-1], dev_loss[-1], dev_acc[-1],\n",
    "    ))\n",
    "    log = {\"tr_loss\": tr_loss, \"tr_acc\": tr_acc, \"dev_loss\": dev_loss, \"dev_acc\": dev_acc}\n",
    "    pickle.dump(log, open(log_folder + \"/log.{}.pickle\".format(epoch), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model to evaluate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = keras.models.load_model(log_folder + \"/model.14.h5\", custom_objects={\"Attention\": Attention})\n",
    "m = keras.models.load_model(log_folder + \"/model.9.h5\", custom_objects={\"tf\": tf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.62352723847417268, 0.76879084967320266)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_evaluate(m, batch_dev_x, batch_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.60254765594644211, 0.78268494900493391)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_evaluate(m, batch_test_x, batch_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test_x[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test_x[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([batch_test_x[0][0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.evaluate(np.array([batch_test_x[0][0]]), np.array([batch_test_y[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.predict(np.array([batch_test_x[0][0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### online inference parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    \"word_index\": word_index, # v\n",
    "    \"inv_label_index\": inv_label_index, # v\n",
    "    \"max_sent_len\": max_sent_len, # v\n",
    "    \"min_word_count\": min_word_count,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"uw_size\": uw_size,\n",
    "    \"us_size\": uw_size,\n",
    "    \"embedding_size\": embedding_size,\n",
    "    \"word_counter\": word_counter,\n",
    "    \"label_index\": label_index,\n",
    "#     \"tr_seq\": tr_seq,\n",
    "#     \"tr_y\": tr_y,\n",
    "#     \"dev_seq\": dev_seq,\n",
    "#     \"dev_y\": dev_y,\n",
    "#     \"test_seq\": test_seq,\n",
    "#     \"test_y\": test_y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(variables, open(log_folder + \"/var.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
